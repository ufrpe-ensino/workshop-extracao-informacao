{"nbformat":4,"nbformat_minor":0,"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.1"},"colab":{"name":"01-PLN-spaCy.ipynb","provenance":[{"file_id":"https://github.com/geoffbacon/nlp-with-nltk-spacy/blob/master/04-spaCy.ipynb","timestamp":1596110087848}],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"8CrK2e4Skshf"},"source":["# spaCy\n","\n","spaCy é uma biblioteca para PLN que se tornado bastante popular nos últimos anos. Ela é mais recente que o NLTK, projetada especificamente para i) trabalhar em problemas maiores e ii) ocultar detalhes irrelevantes para o  usuário. Vamos nos concentrar nos principais recursos:\n","\n","[Carregando modelos pré-treinados](#loading)<br>\n","\n","[Tokenization](#tokenization)<br>\n","\n","[Lemmatization](#lemma)<br>\n","\n","[Named entity recognition (NER)](#ner)<br>\n","\n","[Vizualizando NER](#visualize-ner)<br>\n","\n","[Word vectors e Similaridades](#vectors)<br>\n"]},{"cell_type":"markdown","metadata":{"id":"rndqN2HxleIw"},"source":["# Instalação"]},{"cell_type":"code","metadata":{"id":"8U_RUJ4ZlhGC","colab":{"base_uri":"https://localhost:8080/","height":947},"executionInfo":{"status":"ok","timestamp":1596230365702,"user_tz":180,"elapsed":20665,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"efa8f40a-eecc-4a2a-c3b9-87c39f3bb0c0"},"source":["!pip install spacy\n","!spacy download pt"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.7.1)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (49.2.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n","Collecting pt_core_news_sm==2.2.5\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.2.5/pt_core_news_sm-2.2.5.tar.gz (21.2MB)\n","\u001b[K     |████████████████████████████████| 21.2MB 98.6MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from pt_core_news_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.18.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (0.7.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (4.41.1)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (49.2.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.0.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.0.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.23.0)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.7.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->pt_core_news_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->pt_core_news_sm==2.2.5) (3.1.0)\n","Building wheels for collected packages: pt-core-news-sm\n","  Building wheel for pt-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pt-core-news-sm: filename=pt_core_news_sm-2.2.5-cp36-none-any.whl size=21186282 sha256=1016d62fe89c1a6cbe9c509743ccc3e70c9d59c2a34d4acb1fba98c22c730b37\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-q9dsu0zd/wheels/ea/94/74/ec9be8418e9231b471be5dc7e1b45dd670019a376a6b5bc1c0\n","Successfully built pt-core-news-sm\n","Installing collected packages: pt-core-news-sm\n","Successfully installed pt-core-news-sm-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('pt_core_news_sm')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.6/dist-packages/pt_core_news_sm -->\n","/usr/local/lib/python3.6/dist-packages/spacy/data/pt\n","You can now load the model via spacy.load('pt')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"09htAjC3kshg"},"source":["import os\n","import spacy\n","import numpy as np\n","import pandas as pd"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUa76gWJkshk"},"source":["## Loading spaCy models <a id='loading'></a>\n","\n","O spaCy disponibiliza uma série de [modelos](https://spacy.io/usage/models) pré-treinados, que devem ser baixados e podem funcionar em problemas mais genéricos. spaCy possui modelos diferentes para diferentes idiomas, inclusive em português. \n","\n","Para fazer uso dos modelos, primeiro os carregamos no spaCy com \n","\n","`nlp = spacy.load ('en')`, que armazena o modelo em uma variável chamada nlp para nós. O `'en'` significa inglês. Se você deseja processar dados nesses idiomas, primeiro precisa baixar os modelos relevantes e carregá-los de maneira semelhante.\n","\n","O modelo aqui referenciado para o português `pt` corresponde a um modelo de CNN treinado em um subconjunto da base WikiNER. Além do `pt_core_news_sm`, o spacy possui o `pt_core_news_md` e o `pt_core_news_lg`. Detalhes das bases utilizadas e acurácia de cada modelo podem ser vistos [aqui](https://spacy.io/models/pt). "]},{"cell_type":"code","metadata":{"id":"NYc5VJ3Ikshl"},"source":["nlp = spacy.load('pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PheBKMAgksho"},"source":["O modelo agora pode ser acessado através da variável `nlp`:"]},{"cell_type":"code","metadata":{"id":"YRPytSihn9Ik"},"source":["doc = nlp(\"Eu gostaria que as aboboras viessem com mais sementes.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qZXiXd4ukshu"},"source":["## Tokenization <a id='tokenization'></a>\n","\n","A tokenização no spaCy é bem simples de ser utilizada com os modelos pré-treinados. Quando iteramos sobre um objeto `Doc`, spaCy assume que queremos iterar sobre os tokens."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"uERG0k6Ukshv","colab":{"base_uri":"https://localhost:8080/","height":180},"executionInfo":{"status":"ok","timestamp":1596230369319,"user_tz":180,"elapsed":24256,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"f9e6209e-93de-4107-a543-792ca4a2f024"},"source":["for token in doc:\n","    print(token)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Eu\n","gostaria\n","que\n","as\n","aboboras\n","viessem\n","com\n","mais\n","sementes\n",".\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3ndAAQ6lksh7","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1596230369321,"user_tz":180,"elapsed":24253,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"7ccfa526-d8dc-4b24-c8df-350b55d15675"},"source":["# quantidade de tokens\n","len(doc)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"ZXZzON8rkshy"},"source":["Cada `token` em `doc` é uma instância da classe `Token`. Este objeto armazena uma série de informações relevantes. como a representação em string do token (`.text`) o índice (`.idx`), ou ainda aspectos linguísticos como `.is_stop`,  `.is_space`, `.lemma_`  e `.pos_`."]},{"cell_type":"code","metadata":{"id":"uCKCNrc2kshy","colab":{"base_uri":"https://localhost:8080/","height":180},"executionInfo":{"status":"ok","timestamp":1596230369322,"user_tz":180,"elapsed":24248,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"3ea07e86-288d-4c51-a3e5-1fdfa3c30b33"},"source":["for token in doc:\n","    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n","        token.text,\n","        token.idx,\n","        token.lemma_,\n","        token.is_stop,\n","        token.is_punct,\n","        token.is_space,\n","        token.pos_,\n","        token.dep_,\n","    ))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Eu\t0\tEu\tTrue\tFalse\tFalse\tPRON\tnsubj\n","gostaria\t3\tgostar\tFalse\tFalse\tFalse\tVERB\tROOT\n","que\t12\tque\tTrue\tFalse\tFalse\tSCONJ\tmark\n","as\t16\to\tTrue\tFalse\tFalse\tDET\tdet\n","aboboras\t19\taboborar\tFalse\tFalse\tFalse\tNOUN\tnsubj\n","viessem\t28\tvir\tFalse\tFalse\tFalse\tVERB\tccomp\n","com\t36\tcom\tTrue\tFalse\tFalse\tADP\tcase\n","mais\t40\tmais\tTrue\tFalse\tFalse\tADV\tadvmod\n","sementes\t45\tsemente\tFalse\tFalse\tFalse\tNOUN\tobl\n",".\t53\t.\tFalse\tTrue\tFalse\tPUNCT\tpunct\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"p0llhdANksh-"},"source":["### Exercício\n","\n","Verifique a quantidade de tokens considerados stopwords e pontuação do texto abaixo. Imprima uma versão do texto removendo este tipo de tokens."]},{"cell_type":"code","metadata":{"id":"f6Cvtyplksh-"},"source":["review = \"Text mining, also known as text data mining, is the process of transforming unstructured text into a structured format to identify meaningful patterns and new insights. By applying advanced analytical techniques, such as Naïve Bayes, Support Vector Machines (SVM), and other deep learning algorithms, companies are able to explore and discover hidden relationships within their unstructured data.\"\n","# sua resposta\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkNV8lO_qHpp"},"source":["# Detecção de sentenças\n","\n","As sentenças detectadas pelo modelo ficam armazenadas no atributo `.sents` do objeto `Doc`:"]},{"cell_type":"code","metadata":{"id":"cZf4oy_UqKva"},"source":["doc = nlp(\"Eu estarei em Recife próxima semana. Será que levo roupa para frio?\")\n","for sent in doc.sents:\n","    print(sent)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VBlENetrrEX0"},"source":["### Exercício\n","\n","1. Quantas sentenças existem no texto armazenado na variável `review`?\n","\n","2. Quais são essas sentencas?"]},{"cell_type":"code","metadata":{"id":"n0jZIC-TqY8r"},"source":["# sua resposta\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n0RoWaxJksiC"},"source":["## POS tagging\n","\n","Como vimos acima, ao submeter uma string ao modelo, o spacy aplica todo o pipeline de PLN ao texto, incluindo o processo de POS Tagging:"]},{"cell_type":"code","metadata":{"id":"QeV9D0KHksiC","colab":{"base_uri":"https://localhost:8080/","height":245},"executionInfo":{"status":"ok","timestamp":1596230601038,"user_tz":180,"elapsed":855,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"32a2a43f-0668-476d-fb19-ea3bd35a4d7f"},"source":["doc = nlp(\"Eu estarei em Recife próxima semana. Será que levo roupa para frio?\")\n","for token in doc:\n","    print(token.text, token.pos_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Eu PRON\n","estarei VERB\n","em ADP\n","Recife PROPN\n","próxima ADJ\n","semana NOUN\n",". PUNCT\n","Será VERB\n","que SCONJ\n","levo PROPN\n","roupa VERB\n","para ADP\n","frio NOUN\n","? PUNCT\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"joSrl8nMksiE"},"source":["Esses rótulos podem ser um pouco difíceis de interpretar... O que significa `PROPN`? e `ADP`? Use a função `explain` para obter as respostas:"]},{"cell_type":"code","metadata":{"id":"BTcRmTMbksiF","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1596230601449,"user_tz":180,"elapsed":485,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"f8c4578b-a734-484c-f433-0b8e1f56f4e6"},"source":["print(spacy.explain('PROPN'))\n","print(spacy.explain('ADP'))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["proper noun\n","adposition\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VfEU0kS9ksiH"},"source":["Observe que o texto (label) é sempre armazenado em um atributo com `_` no fim, pois o spaCy armazena internamente tudo na forma de hashes, para tornar o código mais eficiente:"]},{"cell_type":"code","metadata":{"id":"_dNcZc6jksiI","colab":{"base_uri":"https://localhost:8080/","height":180},"executionInfo":{"status":"ok","timestamp":1596230603260,"user_tz":180,"elapsed":845,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"bb38f8a8-7908-4da2-ab84-9283e318bc0c"},"source":["for token in doc[:10]:\n","    print(token.text, token.pos)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Eu 95\n","estarei 100\n","em 85\n","Recife 96\n","próxima 84\n","semana 92\n",". 97\n","Será 100\n","que 98\n","levo 96\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5-Y2TpLyksiL"},"source":["### Exercício\n","\n","Retorne a lista de todas as POS Tags da variável `review` como uma lista de tuplas (word, pos) para cada token no texto. Quais as tags mais frequentes?\n","\n","Dica: você pode utilizar um `pandas.DataFrame` para facilitar as contagens"]},{"cell_type":"code","metadata":{"id":"iRlH28-wksiL"},"source":["# sua resposta\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ngAxxubYxGar","colab":{"base_uri":"https://localhost:8080/","height":277},"executionInfo":{"status":"ok","timestamp":1596230761317,"user_tz":180,"elapsed":694,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"58455cd9-2348-4680-f8d5-bed98894d613"},"source":["df.pos.value_counts()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VERB     25\n","NOUN     24\n","PUNCT    21\n","DET      19\n","ADP      15\n","ADJ      13\n","PRON     13\n","ADV      12\n","PROPN     7\n","AUX       7\n","SYM       3\n","CCONJ     3\n","NUM       2\n","X         2\n","SCONJ     1\n","Name: pos, dtype: int64"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"JlLNgH7GksiN"},"source":["## Lemmatization <a id='lemma'></a>\n","\n","O spaCy não disponibiliza muitos detalhes de escolha de algoritmos para Lemmatization, o que funciona bem para a maioria dos casos, diferentemente do NLTK:"]},{"cell_type":"code","metadata":{"id":"yZkVi_sMksiO","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596230767581,"user_tz":180,"elapsed":857,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"563fecd5-c1b2-4b35-a52f-34dec671ba8d"},"source":["for token in doc:\n","    print(token.text, token.lemma_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Este Este\n","é ser\n","sem sem\n","dúvida dúvida\n","o o\n","pior mal\n","filme filmar\n","que que\n","eu eu\n","já já\n","vi vir\n",". .\n","E E\n","acredite acreditar\n","em em\n","mim mim\n",", ,\n","eu eu\n","vi vir\n","muitos muito\n","filmes filme\n",". .\n","A A\n","reviravolta reviravolta\n","inacreditável inacreditável\n","que que\n","o o\n","filme filmar\n","faz fazer\n","- -\n","passando passar\n","de de\n","um um\n","extremamente extremamente\n","mau mau\n","filme filmar\n","\" \"\n","Formas Formas\n","de de\n","vida vidar\n","alienígenas alienígena\n","habitam habitar\n","a o\n","terra terra\n","\" \"\n",", ,\n","com com\n","um um\n","filme filmar\n","que que\n","tenta tentar\n","espalhar espalhar\n","um um\n","arquicristiano arquicristiano\n","\" \"\n","O O\n","dia dia\n","do do\n","julgamento julgamento\n","está estar\n","próximo próximo\n",", ,\n","buscar buscar\n","Jesus Jesus\n","ou ou\n","queimar queimar\n","por por\n","toda todo\n","a o\n","eternidade eternidade\n","em em\n","as o\n","dívidas dívida\n","ardentes ardente\n","do do\n","inferno infernar\n","\" \"\n","mensagem mensagem\n","- -\n","deixou-me deixou-me\n","atordoado atordoar\n","depois depois\n","de de\n","ter ter\n","sido ser\n","atormentado atormentar\n","por por\n","85 85\n","minutos minuto\n",". .\n","Até Até\n","mesmo mesmo\n","os o\n","cristãos cristão\n","religiosos religioso\n","devem devir\n","se se\n","envergonhar envergonhar\n","ou ou\n","ficar ficar\n","furiosos furioso\n","a o\n","o o\n","ver ver\n","suas suar\n","crenças crença\n","postadas postar\n","d d\n","essa esse\n","maneira maneiro\n",". .\n","Eu Eu\n","não não\n","sabia saber\n","o o\n","que que\n","fazer fazer\n","comigo comigo\n","quando quando\n","assisti assistir\n","a o\n","atuação atuação\n","horrível horrível\n","que que\n","poderia poder\n","ter ter\n","sido ser\n","realizada realizar\n","por por\n","crianças criança\n","de de\n","7 7\n","anos ano\n","de de\n","idade idade\n",". .\n","Simplesmente Simplesmente\n","repugnante repugnante\n",". .\n","Eu Eu\n","não não\n","sou ser\n","cristão cristão\n","nem nem\n","muito muito\n","religioso religioso\n",". .\n","Mas Mas\n","se se\n","eu eu\n","estivesse estar\n",", ,\n","não não\n","teria ter\n","mais mais\n","medo medo\n","do do\n","Inferno Inferno\n",". .\n","Rich Rich\n","Christiano Christiano\n","mostrou mostrar\n","ser ser\n","algo algo\n","muito muito\n","pior mal\n",". .\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8HqUBmnnksiR"},"source":["## Named entity recognition (NER) <a id='ner'></a>\n","\n","Named entity recognition (NER) é uma das principais tarefas em projetos de recuperação e extração de informação em textos. Muitas tarefas se iniciam a partir da detecção de entidades nomeadas, como a extração de relações por exemplo. \n","\n","Em NER, as diferentes entidades nomeadas extraídas são agrupadas por tipo. Por exemplo, \"pessoa\", \"organização\", \"local\", \"país\" etc. No spaCy, existem muitos [tipos diferentes](https://spacy.io/api/annotation#named-entities) de entidades nomeadas que ele pode extrair com modelos pré-treinados.\n","\n","As entidades nomeadas no spaCy estão disponíveis como propriedade `ents` de um` Doc`. O `.label_` nos diz o tipo de entidade nomeada."]},{"cell_type":"code","metadata":{"id":"WIQD7vOTksiS","colab":{"base_uri":"https://localhost:8080/","height":33},"executionInfo":{"status":"ok","timestamp":1596230768120,"user_tz":180,"elapsed":678,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"c6a92c0b-4c7c-4315-ecc1-fd1e63b1ddf1"},"source":["doc = nlp(\"Eu estarei em Recife próxima semana. Será que levo roupa para frio?\")\n","for ent in doc.ents:\n","    print(ent.text,ent.label_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Recife LOC\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hFMZNuh-ksiV","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1596230768764,"user_tz":180,"elapsed":693,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"0660f25d-01a0-425c-fbd0-04709d00eeb1"},"source":["doc = nlp(\"Geraldo Júlio é o prefeito de Recife.\")\n","for ent in doc.ents:\n","    print(ent.text,ent.label_)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Geraldo Júlio PER\n","Recife LOC\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X05yeFFoDiVE","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1596230769237,"user_tz":180,"elapsed":626,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"1dc0dee0-866d-438a-c492-273a64d65a5b"},"source":["#explain também funciona para entidades\n","spacy.explain('LOC')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Non-GPE locations, mountain ranges, bodies of water'"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"yNAqQh_oksiX"},"source":["### Exercício 1\n","\n","Extraia todas as entidades nomeadas da variável `review`."]},{"cell_type":"code","metadata":{"id":"mOR6zb-YksiY","colab":{"base_uri":"https://localhost:8080/","height":82},"executionInfo":{"status":"ok","timestamp":1596230784345,"user_tz":180,"elapsed":716,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"5b846ca7-fd0d-4dbb-a426-8a3637825af8"},"source":["# sua resposta"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Formas de vida alienígenas MISC\n","Jesus PER\n","Inferno MISC\n","Rich Christiano PER\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Lq3jc_2p1avB"},"source":["### Exercício 2\n","Utilizar a lib do python `wikipedia`, para baixar o conteúdo da página referente ao presidente Bolsonaro no wikipedia em português, e analise quais as entidades presentes nas 10 primeiras sentenças do texto."]},{"cell_type":"code","metadata":{"id":"YP_uLoVJksia","colab":{"base_uri":"https://localhost:8080/","height":281},"executionInfo":{"status":"ok","timestamp":1596230791430,"user_tz":180,"elapsed":6125,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"7d372be0-8521-417e-fcfa-ca7da812a936"},"source":["!pip install wikipedia"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting wikipedia\n","  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n","Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n","Building wheels for collected packages: wikipedia\n","  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=743a7fc87282ad4c6846114ed1bec31c6e448a34f1d6b6c32d8b1d9d45c1aa9f\n","  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n","Successfully built wikipedia\n","Installing collected packages: wikipedia\n","Successfully installed wikipedia-1.4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ErHz8BJ-ksic","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1596230793654,"user_tz":180,"elapsed":2013,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"fc2e1ad9-0c85-4e12-d4e6-58a019891a50"},"source":["import wikipedia\n","wikipedia.set_lang(\"pt\")\n","p = wikipedia.page(\"Jair Bolsonaro\")\n","\n","print(p.url)\n","print(p.title)\n","\n","content = p.content "],"execution_count":null,"outputs":[{"output_type":"stream","text":["https://pt.wikipedia.org/wiki/Jair_Bolsonaro\n","Jair Bolsonaro\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TdZ_Bg9q3Hi6"},"source":["# sua resposta\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"43ZXWVoAksie"},"source":["## Visualização NER <a id='visualize-ner'></a>\n","\n","O displaCy é uma extensão do spaCy para visualização do processo de PLN. \n","\n","Após importar a lib `displacy`, podemos usar o método `render` sobre o `doc` criado."]},{"cell_type":"code","metadata":{"id":"WGCp3Pzfksif"},"source":["from spacy import displacy"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"orXlFlfjksii","colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"status":"ok","timestamp":1596231007932,"user_tz":180,"elapsed":802,"user":{"displayName":"André Câmara","photoUrl":"","userId":"07742020742274505109"}},"outputId":"d997fbe9-faeb-4d24-8691-6c624b6a5e82"},"source":["doc = nlp(review)\n","displacy.render(doc, style='ent', jupyter=True)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Este é sem dúvida o pior filme que eu já vi. E acredite em mim, eu vi muitos filmes. A reviravolta inacreditável que o filme faz - passando de um extremamente mau filme &quot;\n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Formas de vida alienígenas\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n","</mark>\n"," habitam a terra&quot;, com um filme que tenta espalhar um arquicristiano &quot;O dia do julgamento está próximo, buscar \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Jesus\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n","</mark>\n"," ou queimar por toda a eternidade em as dívidas ardentes do inferno &quot;mensagem - deixou-me atordoado depois de ter sido atormentado por 85 minutos. Até mesmo os cristãos religiosos devem se envergonhar ou ficar furiosos ao ver suas crenças postadas dessa maneira. Eu não sabia o que fazer comigo quando assisti a atuação horrível que poderia ter sido realizada por crianças de 7 anos de idade. Simplesmente repugnante. Eu não sou cristão nem muito religioso. Mas se eu estivesse, não teria mais medo do \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Inferno\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n","</mark>\n",". \n","<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n","    Rich Christiano\n","    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n","</mark>\n"," mostrou ser algo muito pior.</div></span>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"4GOU_UyY4Srt"},"source":["### Exercício\n","\n","Utilize o `displacy` para visualizar as entidades encontradas em algumas das sentenças do conteúdo da página do wikipedia analisada no exercício anterior."]},{"cell_type":"code","metadata":{"id":"EJhvxC_04kUE"},"source":["# sua resposta"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WyBBECKoksik"},"source":["## Parsing de Dependencia\n","\n","A análise de dependência refere-se a desenhar os relacionamentos entre palavras individuais em uma frase. Assim como o NER, esse é um tópico bem importante em PLN. \n"]},{"cell_type":"code","metadata":{"id":"2SVmTfMFksil"},"source":["for token in doc[:18]:\n","    print(token.text, token.dep_, token.head)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JpimXdJDksio"},"source":["Utilizando o `displacy`:"]},{"cell_type":"code","metadata":{"id":"wwKtQQbUksip"},"source":["displacy.render(doc, style='dep', jupyter=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cld4BFg_54E8"},"source":["# modificando a visualização\n","displacy.render(doc, style='dep', jupyter=True, options={\"compact\": True})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gN6wn9tnksir"},"source":["### Exercício\n","\n","Crie uma lista para cada tipo de entidade no texto da wikipedia analisado, ou seja: \n","- texto (string)\n","- pos\n","- lemma\n","- se é uma stopword (`.is_stop`)\n","- se é pontuação (`.is_punct`)\n","- se é um número (`.like_num`)\n","- a relação de dependência (`.dep_`)\n","\n","Utilize as listas criadas para criar um `pandas.DataFrame`, e analise a distribuição das categorias registradas."]},{"cell_type":"code","metadata":{"id":"65kvgT1eksir"},"source":["doc = nlp(content)\n","tokens = [token.text for token in doc]\n","\n","# resposta\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NRm7I70qksi5"},"source":["## Word vectors e similaridade <a id='vectors'></a>\n","\n","Os modelos do spaCy em sua versão reduzida (`sm`), não incluem word vectors completos, de modo que é possível obter uma aproximação dessas distâncias. Caso se queira utilizar algo mais preciso, é recomendado utilizar os modelos `lg` (large) ou ainda carregar outros dicionários de vetores, como o word2vec, fastText, etc. O `gensim` é uma biblioteca que auxilia o uso de tais dicionários."]},{"cell_type":"code","metadata":{"id":"Vs8k1uYmksi5"},"source":["w1 = nlp('gato')\n","w2 = nlp('cachorro')\n","w3 = nlp('presidente')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXqB_oMhksi7"},"source":["w1.similarity(w2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F6jAu79-8FyW"},"source":["w2.similarity(w3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vRPK-mvfksi9"},"source":["Similaridade entre textos:"]},{"cell_type":"code","metadata":{"id":"Gi_9TrHZksi-"},"source":["doc_review = nlp(review)\n","doc_wiki   = nlp(content)\n","\n","doc_review.similarity(doc_wiki)"],"execution_count":null,"outputs":[]}]}