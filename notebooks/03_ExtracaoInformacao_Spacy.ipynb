{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "08a_ExtracaoInformacao.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NBKYahII-iyZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMPzRXpm-e9_"
      },
      "source": [
        "# Extraindo Entidades Nomeadas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkNdurX0G6z-"
      },
      "source": [
        "## SpaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8hbyepxG87A"
      },
      "source": [
        "import spacy\n",
        "from spacy import displacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cetMeEw-LtKv"
      },
      "source": [
        "### Modelos pré-treinados"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffdMmdHfHnL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f080e166-5182-494b-cb20-27796f820c3a"
      },
      "source": [
        "'''BAIXANDO O MODELO DA LÍNGUA PORTUGUESA'''\n",
        "!python -m spacy download pt"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-15 15:08:29.842265: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'pt' are deprecated. Please use the\n",
            "full pipeline package name 'pt_core_news_sm' instead.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pt-core-news-sm==3.4.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.4.0/pt_core_news_sm-3.4.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.0 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from pt-core-news-sm==3.4.0) (3.4.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.4.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (57.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.9.1)\n",
            "Requirement already satisfied: typing-extensions<4.2.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.21.6)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (4.64.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.3)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.6.2)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (8.1.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.11.3)\n",
            "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.4.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.3.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.8.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (5.2.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2022.6.15)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (0.7.8)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->pt-core-news-sm==3.4.0) (2.0.1)\n",
            "Installing collected packages: pt-core-news-sm\n",
            "Successfully installed pt-core-news-sm-3.4.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMkOesVjHqYA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "outputId": "8c4c97e8-e8aa-42c9-803e-4ca05f49b9c3"
      },
      "source": [
        "'''CLASSIFICANDO CONTEÚDO ATRAVÉS DO MODELO'''\n",
        "\n",
        "text = 'Donald Trump, portador do cpf 064.468.404-62, chegou aos EUA no dia 01/06/2019. Donald Trump visitou Barack Obama e George Bush. Além disso, ele esteve em diversos estados do país pilotando uma ferrari.'\n",
        "\n",
        "my_model = spacy.load('pt')\n",
        "document = my_model(text)\n",
        "\n",
        "print('Original Sentence: %s\\n\\n' % (text))\n",
        "    \n",
        "displacy.render(document, jupyter=True, style='ent')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e1dac00ae444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Donald Trump, portador do cpf 064.468.404-62, chegou aos EUA no dia 01/06/2019. Donald Trump visitou Barack Obama e George Bush. Além disso, ele esteve em diversos estados do país pilotando uma ferrari.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E941] Can't find model 'pt'. It looks like you're trying to load a model from a shortcut, which is obsolete as of spaCy v3.0. To load the model, use its full name instead:\n\nnlp = spacy.load(\"pt_core_news_sm\")\n\nFor more details on the available models, see the models directory: https://spacy.io/models. If you want to create a blank model, use spacy.blank: nlp = spacy.blank(\"pt\")"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQMtoHjVHzlT"
      },
      "source": [
        "### Criando seu próprio modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhZcuWZkHx7w"
      },
      "source": [
        "import random\n",
        "def train_spacy(data,iterations, base_model=None):\n",
        "    TRAIN_DATA = data\n",
        "    if base_model:\n",
        "      nlp = spacy.load(base_model)\n",
        "      print(\"Loaded model '%s'\" % nlp)\n",
        "    else:\n",
        "      nlp = spacy.blank('NER_NESS')      \n",
        "      print('Created blank model')\n",
        "\n",
        "    if \"ner\" not in nlp.pipe_names:\n",
        "        ner = nlp.create_pipe(\"ner\")\n",
        "        nlp.add_pipe(ner, last=True)\n",
        "    # otherwise, get it so we can add labels\n",
        "    else:\n",
        "        ner = nlp.get_pipe(\"ner\")\n",
        "        \n",
        "    # add labels\n",
        "    for _, annotations in TRAIN_DATA:\n",
        "         for ent in annotations.get('entities'):\n",
        "            ner.add_label(ent[2])\n",
        "    \n",
        "    # get names of other pipes to disable them during training\n",
        "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
        "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
        "        optimizer = nlp.begin_training()\n",
        "        for itn in range(iterations):\n",
        "            random.shuffle(TRAIN_DATA)\n",
        "            losses = {}\n",
        "            for text, annotations in TRAIN_DATA:\n",
        "                nlp.update(\n",
        "                    [text],         # batch of texts\n",
        "                    [annotations],  # batch of annotations\n",
        "                    drop=0.2,       # dropout - make it harder to memorise data\n",
        "                    sgd=optimizer,  # callable to update weights\n",
        "                    losses=losses)\n",
        "    return nlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfVgVPTgIAIv"
      },
      "source": [
        "TRAIN_DATA = [('Em 15/07/1988 nasceu essa linda criança', {'entities': [(3, 13, 'Data')]}),\n",
        "              ('Rafael Mello, portador do cpf 064.468.404-62, chegou aos EUA', {'entities': [(0, 12, 'Pessoa'),(30, 44, 'CPF')]}),\n",
        "              ('Data de prisão: 10/01/2018', {'entities': [(16, 26, 'Data')]}),\n",
        "              ('No dia 01/02/2016 foi decretada a setença', {'entities': [(7, 17, 'Data')]}),\n",
        "              ('A data da festa foi 07/05/2019', {'entities': [(20, 30, 'Data')]}),\n",
        "              ('João, portador do cpf 123.456.789-65, foi encontrado', {'entities': [(0, 4, 'PER'), (18, 32, 'CPF')]}),\n",
        "              ('O cpf 025.412.876-99 pertence a Maria', {'entities': [(6, 20, 'CPF')]})\n",
        "             ]\n",
        "           \n",
        "my_model = train_spacy(TRAIN_DATA, 20)\n",
        "document = my_model(text)\n",
        "\n",
        "print('Original Sentence: %s\\n' % (text))\n",
        "displacy.render(document, jupyter=True, style='ent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc3_Pah_J2kx"
      },
      "source": [
        "Se você estiver usando um modelo existente, certifique-se de misturar exemplos de outros tipos de entidade que o spaCy reconheceu corretamente antes. Caso contrário, seu modelo pode aprender o novo tipo, mas “esquecer” o que ele sabia anteriormente. Isso também é conhecido como o problema do “esquecimento catastrófico”."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL3oUyLGIxh3"
      },
      "source": [
        "my_model2 = train_spacy(TRAIN_DATA, 20, 'pt')\n",
        "document = my_model2(text)\n",
        "\n",
        "print('Original Sentence: %s\\n' % (text))\n",
        "displacy.render(document, jupyter=True, style='ent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJKQ7-JSa2Tr"
      },
      "source": [
        "## Adicionando padrões (dicionário)\n",
        "\n",
        "O SpaCy permite que você defina seu próprio dicionário, utilizando ou não funções linguísticas (https://spacy.io/usage/rule-based-matching)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CeM7bDSXH6u"
      },
      "source": [
        "from spacy.matcher import Matcher\n",
        "from spacy.tokens import Span\n",
        "\n",
        "matcher = Matcher(my_model.vocab)\n",
        "matcher.add(\"CARRO\", None, [{\"LOWER\": \"ferrari\"}])\n",
        "\n",
        "doc = my_model(text)\n",
        "matches = matcher(doc)\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    # create a new Span for each match and use the match_id (ANIMAL) as the label\n",
        "    span = Span(doc, start, end, label=match_id)\n",
        "    doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
        "\n",
        "displacy.render(doc, jupyter=True, style='ent')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBKYahII-iyZ"
      },
      "source": [
        "## FLAIR\n",
        "Utilizaremos inicialmente o Flair Framework para nossa tarefa de NER. É uma biblioteca que implementa o estado da arte em NLP atualmente, além de permitir o uso de modelos pré-treinados de forma bastante prática e simples. Ele utiliza o Pytorch como base para definição da sua arquitetura de redes neurais. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DshxgyXC_CmY"
      },
      "source": [
        "### Instalando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "resp3mIq-Kxh"
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_JngOzK_KNU"
      },
      "source": [
        "### Testando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gc2ujFQj_GjQ"
      },
      "source": [
        "#import commands for flair NER\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqDHejdf_OzW"
      },
      "source": [
        "### Modelos Pré-treinados (EN)\n",
        "\n",
        "Você pode ver a lista completa de modelos pré-treinados aqui: https://github.com/flairNLP/flair "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCgfsJgm_I_N"
      },
      "source": [
        "#Load NER Model\n",
        "tagger = SequenceTagger.load('ner')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNEovWge_ZRX"
      },
      "source": [
        "#Sample text to run NER\n",
        "text = 'Jackson is placed in Microsoft located in Redmond'\n",
        "\n",
        "#passing text to sentence\n",
        "sentence = Sentence(text)\n",
        "\n",
        "# Run NER on sentence to identify Entities\n",
        "tagger.predict(sentence)\n",
        "\n",
        "# print the entities with below command\n",
        "for entity in sentence.get_spans('ner'):\n",
        "    print(entity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vXmE4AN_kk8"
      },
      "source": [
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY_HOuRZ_t9n"
      },
      "source": [
        "Testando outra sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JpcbX4r_rCw"
      },
      "source": [
        "#Sample text\n",
        "text1 = 'Redmond is coming to New York city'\n",
        "\n",
        "#passing text to sentence\n",
        "sentence = Sentence(text1)\n",
        "\n",
        "# Run NER on sentence to identify Entities\n",
        "tagger.predict(sentence)\n",
        "\n",
        "# print the entities with below command\n",
        "for entity in sentence.get_spans('ner'):\n",
        "    print(entity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEWpul3aum0Y"
      },
      "source": [
        "print(sentence.to_tagged_string())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aspDysTn_7dd"
      },
      "source": [
        "### Treinando modelo FLAIR em Portugues\n",
        "\n",
        "O treinamento de modelos do FLAIR é bem custoso, e normalmente obtém melhores resultados utilizando embeddings pré-treinados associados a embeddings específicos para fases forward e backward do algoritmo. \n",
        "\n",
        "Até o momento não existem modelos prétreinados disponíveis diretamente no framework, mas algumas iniciativas já estão em andamento (https://github.com/jneto04/ner-pt)"
      ]
    }
  ]
}